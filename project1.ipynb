{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import proj1_helpers as helpers\n",
    "import models\n",
    "import costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tXX, ids = helpers.load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv' # TODO: download train data and supply path here \n",
    "_, tXX_test, ids_test = helpers.load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = tXX_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = tXX.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.vstack([tX, tX_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drope Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(tX.shape[1]):\n",
    "    tmp = tX[:,i]\n",
    "    c = np.count_nonzero(tmp == -999)\n",
    "    #print(f\"Feature {i} | Occurences: {c} | {c/tX.shape[0]}\")\n",
    "    if c/tX.shape[0] > 0.6:\n",
    "        l.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(818238, 26)\n"
     ]
    }
   ],
   "source": [
    "l = [15, 16, 18, 20]\n",
    "data = np.delete(data, l, axis=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tX[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nonexisting(tX):\n",
    "    \"\"\"Replace -999 by median of the row.\"\"\"\n",
    "    tX[tX==-999] = np.NaN\n",
    "    col_median = np.nanmedian(tX, axis=0)\n",
    "    inds = np.where(np.isnan(tX))\n",
    "    tX[inds] = np.take(col_median, inds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tX):\n",
    "    a = np.mean(tX, axis=0)\n",
    "    s = np.std(tX, axis=0)\n",
    "    tX = (tX-a)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(tX):\n",
    "    min_ = np.min(tX, axis=0)\n",
    "    max_ = np.max(tX, axis=0)\n",
    "    tX = (tX - min_) / (max_ - min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(tX):\n",
    "    q75, q25 = np.percentile(tX, [75 ,25])\n",
    "    iqr = q75-q25\n",
    "    lower = q25 - 1.75 * iqr\n",
    "    upper = q75 + 1.75 * iqr\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlierts(tX):\n",
    "    for i in range(tX.shape[1]):\n",
    "        t = tX[:,i]\n",
    "        lower, upper = iqr(t)\n",
    "        t[t < lower] = np.nan\n",
    "        t[t > upper] = np.nan\n",
    "        med = np.nanmedian(t)\n",
    "        t[np.isnan(t)] = med\n",
    "        tX[:,i] = t\n",
    "        \n",
    "def outliers(x, alpha=0):\n",
    "    \"\"\"\n",
    "    Cut the tails: if a value is smaller than alpha_percentile (bigger than 1-alpha_percentile) \n",
    "                   of its features replace it with that percentile\n",
    "    \"\"\"\n",
    "    for i in range(x.shape[1]):\n",
    "        x[:,i][ x[:,i]<np.percentile(x[:,i],alpha) ] = np.percentile(x[:,i],alpha)\n",
    "        x[:,i][ x[:,i]>np.percentile(x[:,i],100-alpha) ] = np.percentile(x[:,i],100-alpha)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_nonexisting(data)\n",
    "#min_max(data)\n",
    "#idx = [0,1,2,5,7,9,10,13,16,19,21,23,26]\n",
    "#a = np.log1p(data[:, idx])\n",
    "#data = np.hstack((data, a))\n",
    "#data = outliers(data)\n",
    "#remove_outlierts(data)\n",
    "normalize(data)\n",
    "#l = [15, 16, 18, 20]\n",
    "#data = np.delete(data, l, axis=1)\n",
    "tX = data[0:250000]\n",
    "tX_test = data[250000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove_outlierts(tX)\n",
    "#tX = normalize(tX)\n",
    "#np.mean(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n",
      "(200000, 301)\n",
      "[-1. -1. -1. ... -1.  1. -1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81642"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed = 1\n",
    "# degree = 7\n",
    "# k_fold = 5\n",
    "# lambdas = [0]\n",
    "# ks = helpers.build_k_indices(y, k_fold, seed)\n",
    "print(y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = helpers.split_data(tX, y, 0.8)\n",
    "degree = 10\n",
    "poly_train = models.build_poly(x_train, degree)\n",
    "poly_test = models.build_poly(x_test, degree)\n",
    "w = models.ridge_regression(y_train, poly_train, 1125e-10)\n",
    "y_l = helpers.predict_labels(w, poly_test)\n",
    "print(y_l)\n",
    "l = y_test - y_l\n",
    "np.count_nonzero(l==0)/l.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = models.calculate_loss(y, tx, w)\n",
    "    grad = models.calculate_gradient(y, tx, w)\n",
    "    w = w - gamma * grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    gamma = -5\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iters in range(max_iter):\n",
    "        #print(iters)\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iters % 10 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iters, l=loss))\n",
    "        # converge criterion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_gradient_descent_demo(y, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g1/pjc7p1j96r3ch7xwklndpx0c0000gn/T/ipykernel_2099/1300908756.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tX' is not defined"
     ]
    }
   ],
   "source": [
    "tX[:100,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'helpers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g1/pjc7p1j96r3ch7xwklndpx0c0000gn/T/ipykernel_5582/383254984.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDATA_TEST_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/test.csv'\u001b[0m \u001b[0;31m# TODO: download train data and supply path here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtXX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_TEST_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtXX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'helpers' is not defined"
     ]
    }
   ],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv' # TODO: download train data and supply path here \n",
    "_, tXX_test, ids_test = helpers.load_csv_data(DATA_TEST_PATH)\n",
    "tX_test = tXX_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = normalize(np.vstack([tX, tX_test]))\n",
    "tX = (tX-m)/s\n",
    "tX_test = (tX_test-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/sample-submission.csv' # TODO: fill in desired name of output file for submission\n",
    "#fix_nonexisting(tX_test)\n",
    "#tX_test = normalize(tX_test)\n",
    "#remove_outlierts(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = models.build_poly(tX_test, 10)\n",
    "y_pred = helpers.predict_labels(w, poly)\n",
    "helpers.create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8171417610226701"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tester import tester\n",
    "\n",
    "tester(OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
